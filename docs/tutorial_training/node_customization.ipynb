{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7661b7ca",
   "metadata": {},
   "source": [
    "# Node Customization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75becb3",
   "metadata": {},
   "source": [
    "@[Chaoming Wang](https://github.com/chaoming0625)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332286a2",
   "metadata": {},
   "source": [
    "To implement a custom node in BrainPy, you will have to write a Python class that subclasses ``brainpy.nn.Node`` and implement several important methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c1e1c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainpy as bp\n",
    "import brainpy.math as bm\n",
    "\n",
    "# bp.math.set_platform('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00534e",
   "metadata": {},
   "source": [
    "## A layer with parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4a556c",
   "metadata": {},
   "source": [
    "If the layer has parameters, these should be initialized in the constructor. In BrainPy, we recommend you to mark parameters as [brainpy.math.TrainVar](../tutorial_math/variables.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a308737",
   "metadata": {},
   "source": [
    "To show how this can be used, here is a layer that multiplies its input by a matrix `W` (much like a typical fully connected layer in a neural network would). This matrix is a parameter of the layer. The shape of the matrix will be *(num_input, num_hidden)*, where *num_input* is the number of input features and *num_hidden* has to be specified when the layer is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96c8fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotLayer(bp.layers.Module):\n",
    "    def __init__(self, num_input, num_hidden, W=bp.initialize.Normal(), **kwargs):\n",
    "        super(DotLayer, self).__init__(**kwargs)\n",
    "        self.num_input = num_input\n",
    "        self.num_hidden = num_hidden\n",
    "        self.W = bm.TrainVar(W([num_input, num_hidden]))\n",
    "\n",
    "    def update(self, x):\n",
    "        return bm.dot(x, self.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a4bcc",
   "metadata": {},
   "source": [
    "A few things are worth noting here: when overriding the constructor, we need to call the superclass constructor on the first line. This is important to ensure the layer functions properly. Note that we pass ``**kwargs`` - although this is not strictly necessary, it enables some other cool features, such as making it possible to give the layer a name: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d654c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_dot = DotLayer(10, 50, name='my_dot_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5acceb",
   "metadata": {},
   "source": [
    "## A layer with multiple behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d17a0",
   "metadata": {},
   "source": [
    "Some layers can have multiple behaviors. For example, a layer implementing *dropout* should be able to be switched on or off. During training, we want it to apply dropout noise to its input and scale up the remaining values, but during evaluation we donâ€™t want it to do anything.\n",
    "\n",
    "For this purpose, the ``update()`` method takes optional keyword arguments (``kwargs``). When ``update()`` is called to compute an expression for the output of a network, all specified keyword arguments are passed to the ``update()`` methods of all layers in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc00ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(bp.layers.Module):\n",
    "    def __init__(self, prob, seed=None, **kwargs):\n",
    "        super(Dropout, self).__init__(**kwargs)\n",
    "        self.prob = prob\n",
    "        self.rng = bm.random.RandomState(seed=seed)\n",
    "\n",
    "    def update(self, x, **kwargs):\n",
    "        if kwargs.get('train', True):\n",
    "            keep_mask = self.rng.bernoulli(self.prob, x.shape)\n",
    "            return bm.where(keep_mask, x / self.prob, 0.)\n",
    "        else:\n",
    "            return x"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
