{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f48e9b",
   "metadata": {},
   "source": [
    "# Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a625b0ab",
   "metadata": {},
   "source": [
    "In this section, we are going to talk about the concept of the code compilation to accelerate your model running performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13e791f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainpy as bp\n",
    "import brainpy.math.jax as bm\n",
    "\n",
    "bp.math.use_backend('jax')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfd2be7",
   "metadata": {},
   "source": [
    "## ``jit()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5848a61e",
   "metadata": {},
   "source": [
    "We have talked about the mechanism of [JIT compilation for class objects in NumPy backend](../quickstart/jit_compilation.html#Mechanism-of-JIT-in-NumPy-backend). In this section, we try to understand how to apply JIT when you are using JAX backend. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123027f3",
   "metadata": {},
   "source": [
    "``jax.jit()`` is excellent, while it only supports [pure functions](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions). ``brainpy.math.jax.jit()`` is based on ``jax.jit()``, but extends its ability to just-in-time compile your class objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a7ad6",
   "metadata": {},
   "source": [
    "### JIT for pure functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541689dd",
   "metadata": {},
   "source": [
    "First, ``brainpy.math.jax.jit()`` can just-in-time compile your functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "354fcfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * bm.where(x > 0, x, alpha * bm.exp(x) - alpha)\n",
    "\n",
    "x = bm.random.normal(size=(1000000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1cf35d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.86 ms ± 136 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit selu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdb2edf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346 µs ± 21.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "selu_jit = bm.jit(selu) # jit accleration\n",
    "\n",
    "%timeit selu_jit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9406eacd",
   "metadata": {},
   "source": [
    "### JIT for class objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae4adb",
   "metadata": {},
   "source": [
    "Moreover, ``brainpy.math.jax.jit()`` is powerful to just-in-time compile your class objects. The [constraints](../quickstart/jit_compilation.ipynb) for class object JIT are:\n",
    "\n",
    "- The JIT target must be a subclass of ``brainpy.Base``.\n",
    "- Dynamically changed variables must be labeled as ``brainpy.math.Variable``.\n",
    "- Variable changes must be made in-place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5374857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(bp.Base):\n",
    "    def __init__(self, dimension):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "\n",
    "        # parameters\n",
    "        self.dimension = dimension\n",
    "\n",
    "        # variables\n",
    "        self.w = bm.Variable(2.0 * bm.ones(dimension) - 1.3)\n",
    "\n",
    "    def __call__(self, X, Y):\n",
    "        u = bm.dot(((1.0 / (1.0 + bm.exp(-Y * bm.dot(X, self.w))) - 1.0) * Y), X)\n",
    "        self.w[:] = self.w - u\n",
    "\n",
    "num_dim, num_points = 10, 200000\n",
    "points = bm.random.random((num_points, num_dim))\n",
    "labels = bm.random.random(num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "346e6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5e5b98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.77 ms ± 140 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit lr(points, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c70b1eba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.29 ms ± 10.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "lr_jit = bm.jit(lr)\n",
    "\n",
    "%timeit lr_jit(points, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a3b576",
   "metadata": {},
   "source": [
    "### JIT mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1046bd0",
   "metadata": {},
   "source": [
    "The mechanism of JIT compilation is that BrainPy automatically transforms your class methods into functions. \n",
    "\n",
    "``brainpy.math.jax.jit()`` receives a ``dyn_vars`` argument, which denotes the dynamically changed variables. If you do not provide it, BrainPy will automatically detect them by calling ``Base.vars()``. Once get \"dyn_vars\", BrainPy will treat \"dyn_vars\" as function arguments, thus making them able to dynamically change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42fbe267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import types\n",
    "\n",
    "isinstance(lr_jit, types.FunctionType)  # \"lr\" is class, while \"lr_jit\" is a function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f0e22",
   "metadata": {},
   "source": [
    "Therefore, the secrete of ``brainpy.math.jax.jit()`` is providing \"dyn_vars\". No matter your target is a class object, a method in the class object, or a pure function, if there are dynamically changed variables, you just pack them into ``brainpy.math.jax.jit()`` as \"dyn_vars\". Then, all the compilation and acceleration will be handled by BrainPy automatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29d5d84",
   "metadata": {},
   "source": [
    "### Example 1: JIT a class method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "076fc88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(bp.Base):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = bm.TrainVar(bm.random.random((n_in, n_out)))\n",
    "        self.b = bm.TrainVar(bm.zeros(n_out))\n",
    "    \n",
    "    def update(self, x):\n",
    "        return x @ self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e5eca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bm.zeros(10)\n",
    "l = Linear(10, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af6f71",
   "metadata": {},
   "source": [
    "This time, we mark \"w\" and \"b\" as dynamically changed variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cca2e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update1 = bm.jit(l.update, dyn_vars=[l.w, l.b])  # make 'w' and 'b' dynamically change\n",
    "update1(x)  # x is 0., b is 0., therefore y is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4c9c2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.b[:] = 1.  # change b to 1, we expect y will be 1 too\n",
    "\n",
    "update1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572c4d8",
   "metadata": {},
   "source": [
    "This time, we only mark \"w\" as dynamically changed variables. We will find also modify \"b\", the results will not change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7bdf120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update2 = bm.jit(l.update, dyn_vars=[l.w])  # make 'w' dynamically change\n",
    "\n",
    "update2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "446ea19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.b[:] = 2.  # change b to 2, while y will not be 2\n",
    "update2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb25ba8",
   "metadata": {},
   "source": [
    "### Example 2: JIT a function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b424f80",
   "metadata": {},
   "source": [
    "Now, we change the above \"Linear\" object to a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "675ce89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = 10;  n_out = 3\n",
    "\n",
    "w = bm.TrainVar(bm.random.random((n_in, n_out)))\n",
    "b = bm.TrainVar(bm.zeros(n_out))\n",
    "\n",
    "def update(x):\n",
    "    return x @ w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9ffb7e",
   "metadata": {},
   "source": [
    "If we do not provide ``dyn_vars``, \"w\" and \"b\" will be compiled as constant values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5e3c1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update1 = bm.jit(update)\n",
    "update1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "922fd101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:] = 1.  # modify the value of 'b' will not \n",
    "           # change the result, because in the \n",
    "           # jitted function, 'b' is already \n",
    "           # a constant\n",
    "update1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbbd220",
   "metadata": {},
   "source": [
    "Provide \"w\" and \"b\" as ``dyn_vars`` will make them dynamically changed again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c301f14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update2 = bm.jit(update, dyn_vars=(w, b))\n",
    "update2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "165bb3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([2., 2., 2.], dtype=float32))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:] = 2.  # change b to 2, while y will not be 2\n",
    "update2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967345db",
   "metadata": {},
   "source": [
    "### RandomState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26106295",
   "metadata": {},
   "source": [
    "We have talked about RandomState in [Variables](./variables.ipynb) section. We said that it is also a Variable. Therefore, if your functions have used the default RandomState (``brainpy.math.jax.random.DEFAULT``), you should add it into the ``dyn_vars`` scope of the function. Otherwise, they will be treated as constants and the jitted function will always return the same value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe1a5925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function():\n",
    "    return bm.random.normal(0, 1, size=(10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93c3d479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                       True,  True], dtype=bool))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = bm.jit(function)\n",
    "\n",
    "f1() == f1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95276b4b",
   "metadata": {},
   "source": [
    "The correct way to make JIT for this function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5dfba12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([False, False, False, False, False, False, False, False,\n",
       "                      False, False], dtype=bool))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.random.seed(1234)\n",
    "\n",
    "f2 = bm.jit(function, dyn_vars=bm.random.DEFAULT)\n",
    "\n",
    "f2() == f2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2c54d3",
   "metadata": {},
   "source": [
    "### Example 3: JIT a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a0425",
   "metadata": {},
   "source": [
    "Now, let's use SGD to train a neural network with JIT acceleration. Here we will use the autograd function ``brainpy.math.jax.grad()``, which will be detailed out in [the next section](./differentiation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b89b7af",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class LinearNet(bp.Base):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(LinearNet, self).__init__()\n",
    "\n",
    "        # weights\n",
    "        self.w = bm.TrainVar(bm.random.random((n_in, n_out)))\n",
    "        self.b = bm.TrainVar(bm.zeros(n_out))\n",
    "        self.r = bm.TrainVar(bm.random.random((n_out, 1)))\n",
    "    \n",
    "    def update(self, x):\n",
    "        h = x @ self.w + self.b\n",
    "        return h @ self.r\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        predict = self.update(x)\n",
    "        return bm.mean((predict - y) ** 2)\n",
    "\n",
    "\n",
    "ln = LinearNet(100, 200)\n",
    "\n",
    "# provide the variables want to update\n",
    "opt = bm.optimizers.SGD(lr=1e-6, train_vars=ln.vars()) \n",
    "\n",
    "# provide the variables require graidents\n",
    "f_grad = bm.grad(ln.loss, grad_vars=ln.vars(), return_value=True)  \n",
    "\n",
    "\n",
    "def train(X, Y):\n",
    "    grads, loss = f_grad(X, Y)\n",
    "    opt.update(grads)\n",
    "    return loss\n",
    "\n",
    "# JIT the train function \n",
    "train_jit = bm.jit(train, dyn_vars=ln.vars() + opt.vars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8ae01dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0, loss = 103.74\n",
      "Train 1, loss = 63.50\n",
      "Train 2, loss = 40.54\n",
      "Train 3, loss = 27.44\n",
      "Train 4, loss = 19.97\n",
      "Train 5, loss = 15.71\n",
      "Train 6, loss = 13.27\n",
      "Train 7, loss = 11.89\n",
      "Train 8, loss = 11.09\n",
      "Train 9, loss = 10.64\n",
      "Train 10, loss = 10.38\n",
      "Train 11, loss = 10.24\n",
      "Train 12, loss = 10.15\n",
      "Train 13, loss = 10.10\n",
      "Train 14, loss = 10.08\n",
      "Train 15, loss = 10.06\n",
      "Train 16, loss = 10.05\n",
      "Train 17, loss = 10.05\n",
      "Train 18, loss = 10.04\n",
      "Train 19, loss = 10.04\n",
      "Train 20, loss = 10.04\n",
      "Train 21, loss = 10.04\n",
      "Train 22, loss = 10.04\n",
      "Train 23, loss = 10.04\n",
      "Train 24, loss = 10.04\n",
      "Train 25, loss = 10.04\n",
      "Train 26, loss = 10.04\n",
      "Train 27, loss = 10.04\n",
      "Train 28, loss = 10.04\n",
      "Train 29, loss = 10.04\n"
     ]
    }
   ],
   "source": [
    "xs = bm.random.random((1000, 100))\n",
    "ys = bm.random.random((1000, 1))\n",
    "\n",
    "for i in range(30):\n",
    "    loss  = train_jit(xs, ys)\n",
    "    print(f'Train {i}, loss = {loss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f23e60",
   "metadata": {},
   "source": [
    "### Static arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf607131",
   "metadata": {},
   "source": [
    "Static arguments are arguments that are treated as static/constant in the jitted function. \n",
    "\n",
    "Numerical arguments used in condition syntax (bool value or resulting bool value), and strings must be marked as static. Otherwise, an error will raise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c624ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@bm.jit\n",
    "def f(x):\n",
    "  if x < 3:  # this will cause error\n",
    "    return 3. * x ** 2\n",
    "  else:\n",
    "    return -4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "43d03199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jax._src.errors.ConcretizationTypeError'> Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\n",
      "The problem arose with the `bool` function. \n",
      "While tracing the function f at <ipython-input-70-14a993a83941>:1 for jit, this concrete value was not available in Python because it depends on the value of the argument 'x'.\n",
      "\n",
      "See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    f(1.)\n",
    "except Exception as e:\n",
    "    print(type(e), e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa080dcc",
   "metadata": {},
   "source": [
    "Simply speaking, arguments resulting boolean values must be declared as static arguments. In ``brainpy.math.jax.jit()`` function, if can set the names of static arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3005cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  if x < 3:  # this will cause error\n",
    "    return 3. * x ** 2\n",
    "  else:\n",
    "    return -4 * x\n",
    "\n",
    "f_jit = bm.jit(f, static_argnames=('x', ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "41349cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(3., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_jit(x=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86485a58",
   "metadata": {},
   "source": [
    "However, it's worthy noting that calling the jitted function with different values for these static arguments will trigger recompilation. Therefore, declaring static arguments may be suitable to the following situations:\n",
    "\n",
    "1. Boolean arguments.\n",
    "2. Arguments only have several possible values. \n",
    "\n",
    "If the argument value change significantly, you'd better not to declare it as static. \n",
    "\n",
    "For more information, please refer to [jax.jit](https://jax.readthedocs.io/en/latest/jax.html?highlight=jit#jax.jit) API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550115c8",
   "metadata": {},
   "source": [
    "## ``vmap()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8f235",
   "metadata": {},
   "source": [
    "Coming soon. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a7c52f",
   "metadata": {},
   "source": [
    "## ``pmap()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b4a77",
   "metadata": {},
   "source": [
    "Coming soon. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
