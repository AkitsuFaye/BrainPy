{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df2aeab",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d521c",
   "metadata": {},
   "source": [
    "@[Chaoming Wang](https://github.com/chaoming0625)\n",
    "@[Xiaoyu Chen](mailto:c-xy17@tsinghua.org.cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f81807",
   "metadata": {},
   "source": [
    "The core idea behind BrainPy is the Just-In-Time (JIT) compilation. JIT compilation enables your Python code to be compiled into machine code \"just-in-time\" for execution. Subsequently, such transformed code can run at native machine code speed!\n",
    "\n",
    "Excellent JIT compilers such as [JAX](https://github.com/google/jax) and [Numba](https://github.com/numba/numba) are provided in Python. While they are designed to work only on [pure Python functions](https://en.wikipedia.org/wiki/Pure_function), most computational neuroscience models have too many parameters and variables to manage using functions only. On the contrary, object-oriented programming (OOP) based on ``class`` in Python makes coding more readable, controlable, flexible, and modular. Therefore, it is necessary to support JIT compilation on class objects for programming in brain modeling. \n",
    "\n",
    "In order to provide **a platform can satisfy the need for brain dynamics programming**, we provide the [brainpy.math](../apis/math.rst) module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fdb1409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainpy as bp\n",
    "import brainpy.math as bm\n",
    "\n",
    "bp.math.set_platform('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f632f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c6cc3d",
   "metadata": {},
   "source": [
    "## Why use ``brainpy.math``?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bff518",
   "metadata": {},
   "source": [
    "Specifically, ``brainpy.math`` makes the following contributions:\n",
    "\n",
    "### 1. **Numpy-like ndarray**.\n",
    "Python users are familiar with [NumPy](https://numpy.org/), especially its [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html). JAX has similar ``ndarray`` structures and operations. However, several basic features are fundamentally different from numpy ndarray. For example, JAX ndarray does not support in-place mutating updates, like ``x[i] += y``. To overcome these drawbacks, ``brainpy.math`` provides ``JaxArray`` that can be used in the same way as numpy ndarray. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed3aff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ndarray in \"numpy\"\n",
    "\n",
    "a = np.arange(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82f9cfb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0] += 5\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c58daa91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0, 1, 2, 3, 4], dtype=int32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ndarray in \"brainpy.math\"\n",
    "\n",
    "b = bm.arange(5)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d2a1bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([5, 1, 2, 3, 4], dtype=int32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0] += 5\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c792b",
   "metadata": {},
   "source": [
    "For more details, please see the [Tensors](./tensors.ipynb) tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627fd723",
   "metadata": {},
   "source": [
    "### 2. **Numpy-like random sampling**. \n",
    "JAX has its own style to make random numbers, which is very different from the original NumPy. To provide a consistent experience, ``brainpy.math`` provides ``brainpy.math.random`` for random sampling just like the ``numpy.random`` module. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca435e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sampling in \"numpy\"\n",
    "\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7d166b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92961609, 0.31637555, 0.18391881, 0.20456028, 0.56772503])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6f71fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90110884, 0.18534658, 2.49626568, 1.53620142, 2.4976073 ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(0., 2., 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdba27e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sampling in \"brainpy.math.random\"\n",
    "\n",
    "bm.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfb052e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0.47887695, 0.5548092 , 0.8850775 , 0.30382073, 0.6007602 ],            dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.random.random(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e63f7e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([-1.5375284 , -0.59702027, -2.2728395 ,  3.2330806 ,\n",
       "                      -0.27385947], dtype=float32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.random.normal(0., 2., 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81903ff6",
   "metadata": {},
   "source": [
    "For more details, please see the [Tensors](./tensors.ipynb) tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af9e385",
   "metadata": {},
   "source": [
    "### 3. **JAX transformations on class objects**. \n",
    "OOP is the essence of Python. However, JAX's excellent tranformations (like JIT compilation) only support [pure functions](https://en.wikipedia.org/wiki/Pure_function). To make them work on object-oriented coding in brain dynamics programming, ``brainpy.math`` extends JAX transformations to Python classess."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4082101e",
   "metadata": {},
   "source": [
    "Example 1: JIT compilation performed on class objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c377055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(bp.Base):\n",
    "    def __init__(self, dimension):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "\n",
    "        # parameters    \n",
    "        self.dimension = dimension\n",
    "    \n",
    "        # variables\n",
    "        self.w = bm.Variable(2.0 * bm.ones(dimension) - 1.3)\n",
    "\n",
    "    def __call__(self, X, Y):\n",
    "        u = bm.dot(((1.0 / (1.0 + bm.exp(-Y * bm.dot(X, self.w))) - 1.0) * Y), X)\n",
    "        self.w[:] = self.w - u\n",
    "        \n",
    "num_dim, num_points = 10, 20000000\n",
    "points = bm.random.random((num_points, num_dim))\n",
    "labels = bm.random.random(num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c423d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 ms ± 25.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "lr1 = LogisticRegression(num_dim)\n",
    "\n",
    "%timeit lr1(points, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0264f0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 ms ± 9.93 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "lr2 = bm.jit(LogisticRegression(num_dim))\n",
    "\n",
    "%timeit lr2(points, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab89d89",
   "metadata": {},
   "source": [
    "Example 2: Autograd performed on variables of a class object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50fb3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(bp.Base):\n",
    "  def __init__(self, num_hidden, num_input, **kwargs):\n",
    "    super(Linear, self).__init__(**kwargs)\n",
    "\n",
    "    # parameters\n",
    "    self.num_input = num_input\n",
    "    self.num_hidden = num_hidden\n",
    "\n",
    "    # variables\n",
    "    self.w = bm.random.random((num_input, num_hidden))\n",
    "    self.b = bm.zeros((num_hidden,))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    r = x @ self.w + self.b\n",
    "    return r.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3996e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Linear(num_hidden=3, num_input=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad95c9ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(JaxArray(DeviceArray([[0.14844148, 0.14844148, 0.14844148],\n",
       "                       [0.2177031 , 0.2177031 , 0.2177031 ]], dtype=float32)),\n",
       " JaxArray(DeviceArray([0.33333334, 0.33333334, 0.33333334], dtype=float32)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.grad(l, grad_vars=(l.w, l.b))(bm.random.random([5, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f217b6",
   "metadata": {},
   "source": [
    "## What is the difference between ``brainpy.math`` and other frameworks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78647e8",
   "metadata": {},
   "source": [
    "``brainpy.math`` is not intended to be a reimplementation of the API of any other frameworks. All we are trying to do is to make **a better brain dynamics programming framework for Python users**. \n",
    "\n",
    "However, there are important differences between ``brainpy.math`` and other frameworks. As is stated above, JAX and many other JAX frameworks follow a functional programming paradigm. When appling this kind of coding style on brain dynamics models, it will become a huge problem due to the overwhelmingly large number of parameters and variables. On the contrary, ``brainpy.math`` allows an object-oriented programming paradigm, which is much more Pythonic. The most similar framework is called [Objax](https://github.com/google/objax) which also supports OOP based on JAX, but it is more suitable for the deep learning domain and not able to be used directly in brain dynamics programming. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a00cad",
   "metadata": {},
   "source": [
    "## How to interoperate `brainpy.math` with other JAX frameworks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c73ac7",
   "metadata": {},
   "source": [
    "`brainpy.math` can be easily interoperated with other JAX frameworks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2f15c5",
   "metadata": {},
   "source": [
    "### 1. data are exchangeable in different frameworks. \n",
    "This can be realized because ``JaxArray`` can be direactly converted to JAX ndarray or NumPy ndarray.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7459094c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([5, 1, 2, 3, 4], dtype=int32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd6508",
   "metadata": {},
   "source": [
    "Convert a ``JaxArray`` into a JAX ndarray. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a38327b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([5, 1, 2, 3, 4], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JaxArray.value is a JAX ndarray\n",
    "b.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1882edee",
   "metadata": {},
   "source": [
    "Convert a ``JaxArray`` into a numpy ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01720ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JaxArray can be easily converted to a numpy ndarray\n",
    "np.asarray(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1191e7",
   "metadata": {},
   "source": [
    "Convert a numpy ndarray into a ``JaxArray``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06e67706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0, 1, 2, 3, 4], dtype=int32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.asarray(np.arange(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f9317",
   "metadata": {},
   "source": [
    "Convert a JAX ndarray into a ``JaxArray``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f4b512b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0, 1, 2, 3, 4], dtype=int32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "bm.asarray(jnp.arange(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f11d43d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0, 1, 2, 3, 4], dtype=int32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.JaxArray(jnp.arange(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5eb21d",
   "metadata": {},
   "source": [
    "### 2. transformations in ``brainpy.math`` also work on functions. \n",
    "APIs in other JAX frameworks can be naturally integrated in BrainPy. Let's take the gradient-based optimization library [Optax](https://github.com/deepmind/optax) as an example to illustrate how to use other JAX frameworks in BrainPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f99dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52731f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create several useful functions. \n",
    "\n",
    "network = bm.vmap(lambda params, x: bm.dot(params, x), in_axes=(None, 0))\n",
    "\n",
    "def compute_loss(params, x, y):\n",
    "  y_pred = network(params, x)\n",
    "  loss = bm.mean(optax.l2_loss(y_pred, y))\n",
    "  return loss\n",
    "\n",
    "@bm.jit\n",
    "def train(params, opt_state, xs, ys):\n",
    "  grads = bm.grad(compute_loss)(params, xs.value, ys)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a630831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "\n",
    "bm.random.seed(42)\n",
    "target_params = 0.5\n",
    "xs = bm.random.normal(size=(16, 2))\n",
    "ys = bm.sum(xs * target_params, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2e02ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters of the model + optimizer\n",
    "\n",
    "params = bm.array([0.0, 0.0])\n",
    "optimizer = optax.adam(learning_rate=1e-1)\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3761f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple update loop\n",
    "\n",
    "for _ in range(1000):\n",
    "  params, opt_state = train(params, opt_state, xs, ys)\n",
    "\n",
    "assert bm.allclose(params, target_params), \\\n",
    "  'Optimization should retrieve the target params used to generate the data.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
