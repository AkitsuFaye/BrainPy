{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55233d4",
   "metadata": {},
   "source": [
    "# Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1e2d7",
   "metadata": {},
   "source": [
    "In this section, we are going to talk about how to make automatical differentiation on your variables in a function or a class object. In nowadays machine learning systems, computing and using gradients are common in various situations. So, we are going to understand \n",
    "\n",
    "- how to  calculate derivatives of arbitrary complex functions, \n",
    "- how to compute high-order gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ae6076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainpy as bp\n",
    "import brainpy.math as bm\n",
    "\n",
    "bp.math.set_platform('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa7421",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca8416",
   "metadata": {},
   "source": [
    "Every autograd function in BrainPy has several keywords. In the below, all examples are illustrated through [brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst). Other autograd functions have the same settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d75313f",
   "metadata": {},
   "source": [
    "### ``argnums`` and ``grad_vars``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772965c3",
   "metadata": {},
   "source": [
    "The autograd functions in BrainPy can compute derivatives of *function arguments* (can be specified through `argnums`) or *non-argument variables* (can be specified through ``grad_vars``). For instance, in this linear readout model, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be17f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = bm.random.random((1, 10))\n",
    "        self.b = bm.zeros(1)\n",
    "    \n",
    "    def update(self, x):\n",
    "        r = bm.dot(self.w, x) + self.b\n",
    "        return r.sum()\n",
    "    \n",
    "l = Linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d93392",
   "metadata": {},
   "source": [
    "If we try to take derivative of the argument \"x\" when calling the update function, we can set this through ``argnums``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf6ae1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0.0940454 , 0.24210012, 0.10360408, 0.2991985 , 0.22486198,\n",
       "                      0.9399384 , 0.88925755, 0.84567535, 0.94881094, 0.7843926 ],            dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = bm.grad(l.update, argnums=0)\n",
    "\n",
    "grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beb97b5",
   "metadata": {},
   "source": [
    "But, if you take care of the derivatives of parameters \"self.w\" and \"self.b\", we should label them with ``grad_vars``:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f0d2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(JaxArray(DeviceArray([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)),\n",
       " JaxArray(DeviceArray([1.], dtype=float32)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = bm.grad(l.update, grad_vars=(l.w, l.b))\n",
    "\n",
    "grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea78df",
   "metadata": {},
   "source": [
    "If we pay attention on the derivatives of both argument \"x\" and parameters \"self.w\" and \"self.b\", ``argnums`` and ``grad_vars`` can be used together. In this time, the gradient function will return gradients with the format of ``(var_grads, arg_grads)``, where ``arg_grads`` refers to the gradients of \"argnums\", and ``var_grads`` refers to the gradients of \"grad_vars\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cc0347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = bm.grad(l.update, grad_vars=(l.w, l.b), argnums=0)\n",
    "\n",
    "var_grads, arg_grads = grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce6f0f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(JaxArray(DeviceArray([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)),\n",
       " JaxArray(DeviceArray([1.], dtype=float32)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa0d8b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0.0940454 , 0.24210012, 0.10360408, 0.2991985 , 0.22486198,\n",
       "                      0.9399384 , 0.88925755, 0.84567535, 0.94881094, 0.7843926 ],            dtype=float32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f20772",
   "metadata": {},
   "source": [
    "### ``return_value``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5b9dd",
   "metadata": {},
   "source": [
    "As you see, autograd function return a function which computes gradients with respect to the function returned value. However, sometimes, we take care of the value that function returns, not just gradients. Therefore, you can set ``return_value=True`` in autograd functions.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "600ea97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = bm.grad(l.update, argnums=0, return_value=True)\n",
    "\n",
    "gradient, value = grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6909c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0.0940454 , 0.24210012, 0.10360408, 0.2991985 , 0.22486198,\n",
       "                      0.9399384 , 0.88925755, 0.84567535, 0.94881094, 0.7843926 ],            dtype=float32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "528b392f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(5.3718853, dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f829bd",
   "metadata": {},
   "source": [
    "### ``has_aux``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f4e2b",
   "metadata": {},
   "source": [
    "In some situations, we are interested in the intermediate values in a function. In this time, ``has_aux=True`` can help you. The constrain is that you must return values with the format of ``(loss, aux_data)``. For instance, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28e93b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAux(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(LinearAux, self).__init__()\n",
    "        self.w = bm.random.random((1, 10))\n",
    "        self.b = bm.zeros(1)\n",
    "    \n",
    "    def update(self, x):\n",
    "        dot = bm.dot(self.w, x)\n",
    "        r = (dot + self.b).sum()\n",
    "        return r, (r, dot)  # here the aux data is a tuple, includes the loss and the dot value.\n",
    "                            # however, aux can be arbitrary complex.\n",
    "    \n",
    "l2 = LinearAux()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c683624",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = bm.grad(l2.update, argnums=0, has_aux=True)\n",
    "\n",
    "gradient, aux = grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "828ae73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0.7740855 , 0.6669129 , 0.74336326, 0.7743118 , 0.08353662,\n",
       "                      0.1557033 , 0.27870536, 0.3860656 , 0.14068758, 0.46460104],            dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d921e0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(4.4679728, dtype=float32),\n",
       " JaxArray(DeviceArray([4.4679728], dtype=float32)))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6becdd17",
   "metadata": {},
   "source": [
    "When multiple keywords (``argnums``, ``grad_vars``, ``has_aux`` or``return_value``) are set simulatenously, the return format of the gradient function can be inspected through the corresponding API documentationm. For the above API, please see [brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b31f4",
   "metadata": {},
   "source": [
    "## ``brainpy.math.grad()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d289c868",
   "metadata": {},
   "source": [
    "[brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst) takes a function/object ($f : \\mathbb{R}^n \\to \\mathbb{R}$) and returns a new function ($\\partial f(x) \\to \\mathbb{R}^n$) which computes the gradient of the original function/object. It's worthy to note that ``brainpy.math.grad()`` only support scalar value return. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56075f51",
   "metadata": {},
   "source": [
    "### Pure functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4cccc",
   "metadata": {},
   "source": [
    "For pure function, the gradient is taken with respect to the first argument: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45352485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b):\n",
    "    return a * 2 + b\n",
    "\n",
    "grad_f1 = bm.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6009405f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2., dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_f1(2., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06f4f4e",
   "metadata": {},
   "source": [
    "However, this can be controlled via the `argnums` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58aa6fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(2., dtype=float32), DeviceArray(1., dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_f2 = bm.grad(f, argnums=(0, 1))\n",
    "\n",
    "grad_f2(2., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0874ef",
   "metadata": {},
   "source": [
    "### Class objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00906f22",
   "metadata": {},
   "source": [
    "For a class object or a class bound function, the gradient is taken with respect to the provided ``grad_vars`` and ``argnums`` setting: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acc95d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(F, self).__init__()\n",
    "        self.a = bm.TrainVar(bm.ones(1))\n",
    "        self.b = bm.TrainVar(bm.ones(1))\n",
    "\n",
    "    def __call__(self, c):\n",
    "        ab = self.a * self.b\n",
    "        ab2 = ab * 2\n",
    "        vv = ab2 + c\n",
    "        return vv.mean()\n",
    "    \n",
    "f = F()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d64bc3",
   "metadata": {},
   "source": [
    "The ``grad_vars`` can be a JaxArray, or a list/tuple/dict of JaxArray. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30484eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F0.a': TrainVar(DeviceArray([2.], dtype=float32)),\n",
       " 'F0.b': TrainVar(DeviceArray([2.], dtype=float32))}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.grad(f, grad_vars=f.train_vars())(10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa99d3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TrainVar(DeviceArray([2.], dtype=float32)),\n",
       " TrainVar(DeviceArray([2.], dtype=float32))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.grad(f, grad_vars=[f.a, f.b])(10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847c77f",
   "metadata": {},
   "source": [
    "If there are values dynamically changed in the gradient function, you can provide them in the ``dyn_vars`` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f77b4c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F2(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(F2, self).__init__()\n",
    "        self.a = bm.TrainVar(bm.ones(1))\n",
    "        self.b = bm.TrainVar(bm.ones(1))\n",
    "\n",
    "    def __call__(self, c):\n",
    "        ab = self.a * self.b\n",
    "        ab = ab * 2\n",
    "        self.a.value = ab\n",
    "        return (ab + c).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0cf62b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainVar(DeviceArray([2.], dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2 = F2()\n",
    "bm.grad(f2, dyn_vars=[f2.a], grad_vars=f2.b)(10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6998ec7c",
   "metadata": {},
   "source": [
    "Also, if you are interested with the gradient of the input value, please use ``argnums`` argument. For this situation, calling the gradient function will return ``(grads_of_grad_vars, grads_of_args)``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42c0dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F3(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(F3, self).__init__()\n",
    "        self.a = bm.TrainVar(bm.ones(1))\n",
    "        self.b = bm.TrainVar(bm.ones(1))\n",
    "\n",
    "    def __call__(self, c, d):\n",
    "        ab = self.a * self.b\n",
    "        ab = ab * 2\n",
    "        return (ab + c * d).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fe1c9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads_of_gv : [TrainVar(DeviceArray([2.], dtype=float32)), TrainVar(DeviceArray([2.], dtype=float32))]\n",
      "grads_of_args : 3.0\n"
     ]
    }
   ],
   "source": [
    "f3 = F3()\n",
    "grads_of_gv, grad_of_args = bm.grad(f3, grad_vars=[f3.a, f3.b], argnums=0)(10., 3.)\n",
    "\n",
    "print(\"grads_of_gv :\", grads_of_gv)\n",
    "print(\"grad_of_args :\", grad_of_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba55cac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads_of_gv : [TrainVar(DeviceArray([2.], dtype=float32)), TrainVar(DeviceArray([2.], dtype=float32))]\n",
      "grad_of_arg0 : 3.0\n",
      "grad_of_arg1 : 10.0\n"
     ]
    }
   ],
   "source": [
    "f3 = F3()\n",
    "grads_of_gv, grad_of_args = bm.grad(f3, grad_vars=[f3.a, f3.b], argnums=(0, 1))(10., 3.)\n",
    "\n",
    "print(\"grads_of_gv :\", grads_of_gv)\n",
    "print(\"grad_of_args :\", grad_of_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06491457",
   "metadata": {},
   "source": [
    "Actually, we recommend you to provide any dynamically changed variables (no matter them are updated in the gradient function) in the ``dyn_vars`` argument. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cedb4d",
   "metadata": {},
   "source": [
    "### Auxiliary data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1469a67f",
   "metadata": {},
   "source": [
    "Usually, we want to get the value of the loss, or, we want to return some intermediate variables during the gradient computation. For these situation, users can set ``has_aux=True`` to return auxiliary data, and set ``return_value=True`` to return loss value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a34a7e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  TrainVar(DeviceArray([2.], dtype=float32))\n",
      "loss:  12.0\n"
     ]
    }
   ],
   "source": [
    "# return loss\n",
    "\n",
    "grad, loss = bm.grad(f, grad_vars=f.a, return_value=True)(10.)\n",
    "\n",
    "print('grad: ', grad)\n",
    "print('loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a1ad862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  TrainVar(DeviceArray([1.], dtype=float32))\n",
      "aux_data:  (JaxArray(DeviceArray([1.], dtype=float32)), JaxArray(DeviceArray([2.], dtype=float32)))\n"
     ]
    }
   ],
   "source": [
    "class F4(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(F4, self).__init__()\n",
    "        self.a = bm.TrainVar(bm.ones(1))\n",
    "        self.b = bm.TrainVar(bm.ones(1))\n",
    "\n",
    "    def __call__(self, c):\n",
    "        ab = self.a * self.b\n",
    "        ab2 = ab * 2\n",
    "        loss = (ab + c).mean()\n",
    "        return loss, (ab, ab2)\n",
    "    \n",
    "\n",
    "f4 = F4()\n",
    "    \n",
    "# return intermediate values\n",
    "grad, aux_data = bm.grad(f4, grad_vars=f4.a, has_aux=True)(10.)\n",
    "\n",
    "print('grad: ', grad)\n",
    "print('aux_data: ', aux_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2c322",
   "metadata": {},
   "source": [
    "```note\n",
    "Any function wants to compute gradients through ``brainpy.math.grad()`` must return a scalar value. Otherwise an error will raise. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea6a89f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'TypeError'> Gradient only defined for scalar-output functions. Output was [0. 0.].\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bm.grad(lambda x: x)(bm.zeros(2))\n",
    "except Exception as e:\n",
    "    print(type(e), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d08e3753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0.5, 0.5], dtype=float32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is right\n",
    "\n",
    "bm.grad(lambda x: x.mean())(bm.zeros(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119967c0",
   "metadata": {},
   "source": [
    "## ``brainpy.math.vector_grad()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542356e",
   "metadata": {},
   "source": [
    "If you want to take gradients for a vector-output values, please use ``brainpy.math.jacobian()`` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0a9b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a776e614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ca257d2",
   "metadata": {},
   "source": [
    "## ``brainpy.math.jacobian()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68747a3",
   "metadata": {},
   "source": [
    "Coming soon."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainpy",
   "language": "python",
   "name": "brainpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
