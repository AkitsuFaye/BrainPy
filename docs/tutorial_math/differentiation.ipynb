{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55233d4",
   "metadata": {},
   "source": [
    "# Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1e2d7",
   "metadata": {},
   "source": [
    "In this section, we are going to talk about how to make automatical differentiation on your variables in a function or a class object. In nowadays machine learning systems, computing and using gradients are common in various situations. So, we are going to understand \n",
    "\n",
    "- how to  calculate derivatives of arbitrary complex functions, \n",
    "- how to compute high-order gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ae6076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainpy as bp\n",
    "import brainpy.math as bm\n",
    "\n",
    "bp.math.set_platform('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa7421",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca8416",
   "metadata": {},
   "source": [
    "Every autograd function in BrainPy has several keywords. In the below, all examples are illustrated through [brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst). Other autograd functions have the same settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d75313f",
   "metadata": {},
   "source": [
    "### ``argnums`` and ``grad_vars``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772965c3",
   "metadata": {},
   "source": [
    "The autograd functions in BrainPy can compute derivatives of *function arguments* (can be specified through `argnums`) or *non-argument variables* (can be specified through ``grad_vars``). For instance, in this linear readout model, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be17f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = bm.random.random((1, 10))\n",
    "        self.b = bm.zeros(1)\n",
    "    \n",
    "    def update(self, x):\n",
    "        r = bm.dot(self.w, x) + self.b\n",
    "        return r.sum()\n",
    "    \n",
    "l = Linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d93392",
   "metadata": {},
   "source": [
    "If we try to take derivative of the argument \"x\" when calling the update function, we can set this through ``argnums``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf6ae1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0.0940454 , 0.24210012, 0.10360408, 0.2991985 , 0.22486198,\n",
       "                      0.9399384 , 0.88925755, 0.84567535, 0.94881094, 0.7843926 ],            dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = bm.grad(l.update, argnums=0)\n",
    "\n",
    "grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beb97b5",
   "metadata": {},
   "source": [
    "But, if you take care of the derivatives of parameters \"self.w\" and \"self.b\", we should label them with ``grad_vars``:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f0d2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(JaxArray(DeviceArray([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)),\n",
       " JaxArray(DeviceArray([1.], dtype=float32)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = bm.grad(l.update, grad_vars=(l.w, l.b))\n",
    "\n",
    "grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea78df",
   "metadata": {},
   "source": [
    "If we pay attention on the derivatives of both argument \"x\" and parameters \"self.w\" and \"self.b\", ``argnums`` and ``grad_vars`` can be used together. In this time, the gradient function will return gradients with the format of ``(var_grads, arg_grads)``, where ``arg_grads`` refers to the gradients of \"argnums\", and ``var_grads`` refers to the gradients of \"grad_vars\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cc0347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = bm.grad(l.update, grad_vars=(l.w, l.b), argnums=0)\n",
    "\n",
    "var_grads, arg_grads = grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce6f0f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(JaxArray(DeviceArray([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)),\n",
       " JaxArray(DeviceArray([1.], dtype=float32)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa0d8b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0.0940454 , 0.24210012, 0.10360408, 0.2991985 , 0.22486198,\n",
       "                      0.9399384 , 0.88925755, 0.84567535, 0.94881094, 0.7843926 ],            dtype=float32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f20772",
   "metadata": {},
   "source": [
    "### ``return_value``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5b9dd",
   "metadata": {},
   "source": [
    "As you see, autograd function return a function which computes gradients with respect to the function returned value. However, sometimes, we take care of the value that function returns, not just gradients. Therefore, you can set ``return_value=True`` in autograd functions.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "600ea97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = bm.grad(l.update, argnums=0, return_value=True)\n",
    "\n",
    "gradient, value = grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6909c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0.0940454 , 0.24210012, 0.10360408, 0.2991985 , 0.22486198,\n",
       "                      0.9399384 , 0.88925755, 0.84567535, 0.94881094, 0.7843926 ],            dtype=float32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "528b392f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(5.3718853, dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f829bd",
   "metadata": {},
   "source": [
    "### ``has_aux``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f4e2b",
   "metadata": {},
   "source": [
    "In some situations, we are interested in the intermediate values in a function. In this time, ``has_aux=True`` can help you. The constrain is that you must return values with the format of ``(loss, aux_data)``. For instance, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28e93b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAux(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(LinearAux, self).__init__()\n",
    "        self.w = bm.random.random((1, 10))\n",
    "        self.b = bm.zeros(1)\n",
    "    \n",
    "    def update(self, x):\n",
    "        dot = bm.dot(self.w, x)\n",
    "        r = (dot + self.b).sum()\n",
    "        return r, (r, dot)  # here the aux data is a tuple, includes the loss and the dot value.\n",
    "                            # however, aux can be arbitrary complex.\n",
    "    \n",
    "l2 = LinearAux()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c683624",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = bm.grad(l2.update, argnums=0, has_aux=True)\n",
    "\n",
    "gradient, aux = grad(bm.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "828ae73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0.7740855 , 0.6669129 , 0.74336326, 0.7743118 , 0.08353662,\n",
       "                      0.1557033 , 0.27870536, 0.3860656 , 0.14068758, 0.46460104],            dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d921e0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(4.4679728, dtype=float32),\n",
       " JaxArray(DeviceArray([4.4679728], dtype=float32)))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6becdd17",
   "metadata": {},
   "source": [
    "When multiple keywords (``argnums``, ``grad_vars``, ``has_aux`` or``return_value``) are set simulatenously, the return format of the gradient function can be inspected through the corresponding API documentationm. For the above API, please see [brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b31f4",
   "metadata": {},
   "source": [
    "## ``brainpy.math.grad()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d289c868",
   "metadata": {},
   "source": [
    "[brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst) takes a function/object ($f : \\mathbb{R}^n \\to \\mathbb{R}$) and returns a new function ($\\partial f(x) \\to \\mathbb{R}^n$) which computes the gradient of the original function/object. It's worthy to note that ``brainpy.math.grad()`` only support scalar value return. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56075f51",
   "metadata": {},
   "source": [
    "### Pure functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4cccc",
   "metadata": {},
   "source": [
    "For pure function, the gradient is taken with respect to the first argument: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45352485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b):\n",
    "    return a * 2 + b\n",
    "\n",
    "grad_f1 = bm.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6009405f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2., dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_f1(2., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06f4f4e",
   "metadata": {},
   "source": [
    "However, this can be controlled via the `argnums` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58aa6fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(2., dtype=float32), DeviceArray(1., dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_f2 = bm.grad(f, argnums=(0, 1))\n",
    "\n",
    "grad_f2(2., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0874ef",
   "metadata": {},
   "source": [
    "### Class objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00906f22",
   "metadata": {},
   "source": [
    "For a class object or a class bound function, the gradient is taken with respect to the provided ``grad_vars`` and ``argnums`` setting: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acc95d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(F, self).__init__()\n",
    "        self.a = bm.TrainVar(bm.ones(1))\n",
    "        self.b = bm.TrainVar(bm.ones(1))\n",
    "\n",
    "    def __call__(self, c):\n",
    "        ab = self.a * self.b\n",
    "        ab2 = ab * 2\n",
    "        vv = ab2 + c\n",
    "        return vv.mean()\n",
    "    \n",
    "f = F()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d64bc3",
   "metadata": {},
   "source": [
    "The ``grad_vars`` can be a JaxArray, or a list/tuple/dict of JaxArray. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30484eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F0.a': TrainVar(DeviceArray([2.], dtype=float32)),\n",
       " 'F0.b': TrainVar(DeviceArray([2.], dtype=float32))}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.grad(f, grad_vars=f.train_vars())(10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa99d3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TrainVar(DeviceArray([2.], dtype=float32)),\n",
       " TrainVar(DeviceArray([2.], dtype=float32))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.grad(f, grad_vars=[f.a, f.b])(10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847c77f",
   "metadata": {},
   "source": [
    "If there are values dynamically changed in the gradient function, you can provide them in the ``dyn_vars`` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f77b4c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F2(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(F2, self).__init__()\n",
    "        self.a = bm.TrainVar(bm.ones(1))\n",
    "        self.b = bm.TrainVar(bm.ones(1))\n",
    "\n",
    "    def __call__(self, c):\n",
    "        ab = self.a * self.b\n",
    "        ab = ab * 2\n",
    "        self.a.value = ab\n",
    "        return (ab + c).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0cf62b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainVar(DeviceArray([2.], dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2 = F2()\n",
    "bm.grad(f2, dyn_vars=[f2.a], grad_vars=f2.b)(10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6998ec7c",
   "metadata": {},
   "source": [
    "Also, if you are interested with the gradient of the input value, please use ``argnums`` argument. For this situation, calling the gradient function will return ``(grads_of_grad_vars, grads_of_args)``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42c0dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F3(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(F3, self).__init__()\n",
    "        self.a = bm.TrainVar(bm.ones(1))\n",
    "        self.b = bm.TrainVar(bm.ones(1))\n",
    "\n",
    "    def __call__(self, c, d):\n",
    "        ab = self.a * self.b\n",
    "        ab = ab * 2\n",
    "        return (ab + c * d).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fe1c9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads_of_gv : [TrainVar(DeviceArray([2.], dtype=float32)), TrainVar(DeviceArray([2.], dtype=float32))]\n",
      "grads_of_args : 3.0\n"
     ]
    }
   ],
   "source": [
    "f3 = F3()\n",
    "grads_of_gv, grad_of_args = bm.grad(f3, grad_vars=[f3.a, f3.b], argnums=0)(10., 3.)\n",
    "\n",
    "print(\"grads_of_gv :\", grads_of_gv)\n",
    "print(\"grad_of_args :\", grad_of_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba55cac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads_of_gv : [TrainVar(DeviceArray([2.], dtype=float32)), TrainVar(DeviceArray([2.], dtype=float32))]\n",
      "grad_of_arg0 : 3.0\n",
      "grad_of_arg1 : 10.0\n"
     ]
    }
   ],
   "source": [
    "f3 = F3()\n",
    "grads_of_gv, grad_of_args = bm.grad(f3, grad_vars=[f3.a, f3.b], argnums=(0, 1))(10., 3.)\n",
    "\n",
    "print(\"grads_of_gv :\", grads_of_gv)\n",
    "print(\"grad_of_args :\", grad_of_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06491457",
   "metadata": {},
   "source": [
    "Actually, we recommend you to provide any dynamically changed variables (no matter them are updated in the gradient function) in the ``dyn_vars`` argument. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cedb4d",
   "metadata": {},
   "source": [
    "### Auxiliary data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1469a67f",
   "metadata": {},
   "source": [
    "Usually, we want to get the value of the loss, or, we want to return some intermediate variables during the gradient computation. For these situation, users can set ``has_aux=True`` to return auxiliary data, and set ``return_value=True`` to return loss value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a34a7e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  TrainVar(DeviceArray([2.], dtype=float32))\n",
      "loss:  12.0\n"
     ]
    }
   ],
   "source": [
    "# return loss\n",
    "\n",
    "grad, loss = bm.grad(f, grad_vars=f.a, return_value=True)(10.)\n",
    "\n",
    "print('grad: ', grad)\n",
    "print('loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a1ad862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:  TrainVar(DeviceArray([1.], dtype=float32))\n",
      "aux_data:  (JaxArray(DeviceArray([1.], dtype=float32)), JaxArray(DeviceArray([2.], dtype=float32)))\n"
     ]
    }
   ],
   "source": [
    "class F4(bp.Base):\n",
    "    def __init__(self):\n",
    "        super(F4, self).__init__()\n",
    "        self.a = bm.TrainVar(bm.ones(1))\n",
    "        self.b = bm.TrainVar(bm.ones(1))\n",
    "\n",
    "    def __call__(self, c):\n",
    "        ab = self.a * self.b\n",
    "        ab2 = ab * 2\n",
    "        loss = (ab + c).mean()\n",
    "        return loss, (ab, ab2)\n",
    "    \n",
    "\n",
    "f4 = F4()\n",
    "    \n",
    "# return intermediate values\n",
    "grad, aux_data = bm.grad(f4, grad_vars=f4.a, has_aux=True)(10.)\n",
    "\n",
    "print('grad: ', grad)\n",
    "print('aux_data: ', aux_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2c322",
   "metadata": {},
   "source": [
    "```note\n",
    "Any function wants to compute gradients through ``brainpy.math.grad()`` must return a scalar value. Otherwise an error will raise. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea6a89f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'TypeError'> Gradient only defined for scalar-output functions. Output was [0. 0.].\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bm.grad(lambda x: x)(bm.zeros(2))\n",
    "except Exception as e:\n",
    "    print(type(e), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d08e3753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0.5, 0.5], dtype=float32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is right\n",
    "\n",
    "bm.grad(lambda x: x.mean())(bm.zeros(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119967c0",
   "metadata": {},
   "source": [
    "## ``brainpy.math.vector_grad()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542356e",
   "metadata": {},
   "source": [
    "If you want to take gradients for a vector-output values, please use [brainpy.math.vector_grad()](../apis/auto/math/generated/brainpy.math.autograd.vector_grad.rst)  function. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a0a9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b): \n",
    "    return bm.sin(b) * a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb68361",
   "metadata": {},
   "source": [
    "Gradients for vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1323e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors\n",
    "\n",
    "a = bm.arange(5.)\n",
    "b = bm.random.random(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a776e614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([0.829829  , 0.3382971 , 0.13563846, 0.5101524 , 0.28861028],            dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(f)(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85748195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(JaxArray(DeviceArray([0.829829  , 0.3382971 , 0.13563846, 0.5101524 , 0.28861028],            dtype=float32)),\n",
       " JaxArray(DeviceArray([0.       , 0.9410394, 1.9815168, 2.580252 , 3.8297865], dtype=float32)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(f, argnums=(0, 1))(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10694945",
   "metadata": {},
   "source": [
    "Gradients for matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19acd682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix\n",
    "\n",
    "a = bm.arange(6.).reshape((2, 3))\n",
    "b = bm.random.random((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c049c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([[0.       , 0.6934817, 1.9375703],\n",
       "                      [2.142562 , 2.5830717, 4.9865813]], dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(f, argnums=1)(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "060fb4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(JaxArray(DeviceArray([[0.09120136, 0.72047424, 0.24790175],\n",
       "                       [0.6999546 , 0.7635338 , 0.07321358]], dtype=float32)),\n",
       " JaxArray(DeviceArray([[0.       , 0.6934817, 1.9375703],\n",
       "                       [2.142562 , 2.5830717, 4.9865813]], dtype=float32)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(f, argnums=(0, 1))(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e96324",
   "metadata": {},
   "source": [
    "Similar to [brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst) , ``brainpy.math.vector_grad()`` also supports take derivatives of variables in a class object. Here we show an simple example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34e4f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(bp.Base):\n",
    "  def __init__(self):\n",
    "    super(Test, self).__init__()\n",
    "    self.x = bm.ones(5)\n",
    "    self.y = bm.ones(5)\n",
    "\n",
    "  def __call__(self):\n",
    "    return self.x ** 2 + self.y ** 3 + 10\n",
    "\n",
    "t = Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91fb638c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([2., 2., 2., 2., 2.], dtype=float32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(t, grad_vars=t.x)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "678e2a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(JaxArray(DeviceArray([2., 2., 2., 2., 2.], dtype=float32)),)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(t, grad_vars=(t.x, ))()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3279ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(JaxArray(DeviceArray([2., 2., 2., 2., 2.], dtype=float32)),\n",
       " JaxArray(DeviceArray([3., 3., 3., 3., 3.], dtype=float32)))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.vector_grad(t, grad_vars=(t.x, t.y))()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b9cb39",
   "metadata": {},
   "source": [
    "Other supports like ``return_value`` or ``has_aux`` in [brainpy.math.vector_grad()](../apis/auto/math/generated/brainpy.math.autograd.vector_grad.rst)  are the same with [brainpy.math.grad()](../apis/auto/math/generated/brainpy.math.autograd.grad.rst) ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca257d2",
   "metadata": {},
   "source": [
    "## ``brainpy.math.jacobian()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68747a3",
   "metadata": {},
   "source": [
    "Another way to take gradients of a vector-output value is using [brainpy.math.jacobian()](../apis/auto/math/generated/brainpy.math.autograd.jacobian.rst). ``brainpy.math.jacobian()`` aims to automatically compute the Jacobian matrices $\\partial f(x) \\in \\mathbb{R}^{m \\times n}$ by the given function $f : \\mathbb{R}^n \\to \\mathbb{R}^m$ at the given point of $x \\in \\mathbb{R}^n$. Here, we will not go to the details of the implementation and usage of the ``brainpy.math.jacobian()`` function. Instead, we only show two examples will deliveried on the pure function and class function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253df55c",
   "metadata": {},
   "source": [
    "Given the following function, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13ff570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def f1(x, y):\n",
    "    a = 4 * x[1] ** 2 - 2 * x[2]\n",
    "    r = jnp.asarray([x[0] * y[0], 5 * x[2] * y[1], a, x[2] * jnp.sin(x[0])])\n",
    "    return r, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1aefb47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = bm.array([1., 2., 3.])\n",
    "_y = bm.array([10., 5.])\n",
    "    \n",
    "grads, vec, aux = bm.jacobian(f1, return_value=True, has_aux=True)(_x, _y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6ea00cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([[10.        ,  0.        ,  0.        ],\n",
       "                      [ 0.        ,  0.        , 25.        ],\n",
       "                      [ 0.        , 16.        , -2.        ],\n",
       "                      [ 1.6209068 ,  0.        ,  0.84147096]], dtype=float32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c08984b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([10.       , 75.       , 10.       ,  2.5244129], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b64116c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(10., dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad1eae",
   "metadata": {},
   "source": [
    "Given the following class objects,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f451a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(bp.Base):\n",
    "  def __init__(self):\n",
    "    super(Test, self).__init__()\n",
    "    self.x = bm.array([1., 2., 3.])\n",
    "\n",
    "  def __call__(self, y):\n",
    "    a = self.x[0] * y[0]\n",
    "    b = 5 * self.x[2] * y[1]\n",
    "    c = 4 * self.x[1] ** 2 - 2 * self.x[2]\n",
    "    d = self.x[2] * jnp.sin(self.x[0])\n",
    "    r = jnp.asarray([a, b, c, d])\n",
    "    return r, (c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f68ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Test()\n",
    "f_grad = bm.jacobian(t, grad_vars=t.x, argnums=0, has_aux=True, return_value=True)\n",
    "\n",
    "(var_grads, arg_grads), value, aux = f_grad(_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3db0d7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([[10.        ,  0.        ,  0.        ],\n",
       "                      [ 0.        ,  0.        , 25.        ],\n",
       "                      [ 0.        , 16.        , -2.        ],\n",
       "                      [ 1.6209068 ,  0.        ,  0.84147096]], dtype=float32))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82547a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JaxArray(DeviceArray([[ 1.,  0.],\n",
       "                      [ 0., 15.],\n",
       "                      [ 0.,  0.],\n",
       "                      [ 0.,  0.]], dtype=float32))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "382e1ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([10.       , 75.       , 10.       ,  2.5244129], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de401f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(10., dtype=float32), DeviceArray(2.5244129, dtype=float32))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a486499",
   "metadata": {},
   "source": [
    "More automatical differentation APIs please see our [API documentation](../apis/auto/math/autograd.rst)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainpy",
   "language": "python",
   "name": "brainpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
