Activation Functions
====================

.. currentmodule:: brainpy.math.jax.activations
.. automodule:: brainpy.math.jax.activations


.. autosummary::
    :toctree: generated/

    celu
    elu
    gelu
    glu
    hard_tanh
    hard_sigmoid
    hard_silu
    hard_swish
    leaky_relu
    log_sigmoid
    log_softmax
    one_hot
    normalize
    relu
    relu6
    sigmoid
    soft_sign
    softmax
    softplus
    silu
    swish
    selu
    tanh
